{"Title": "Changes in neuronal representations of consonants in the ascending auditory system and their role in speech recognition", "Year": 2018, "Source": "Front. Neurosci.", "Volume": "12", "Issue": null, "Art.No": null, "PageStart": null, "PageEnd": null, "CitedBy": 5, "DOI": "10.3389/fnins.2018.00671", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85055184025&origin=inward", "Abstract": "Copyright \u00a9 2018 Steadman and Sumner.A fundamental task of the ascending auditory system is to produce representations that facilitate the recognition of complex sounds. This is particularly challenging in the context of acoustic variability, such as that between different talkers producing the same phoneme. These representations are transformed as information is propagated throughout the ascending auditory system from the inner ear to the auditory cortex (AI). Investigating these transformations and their role in speech recognition is key to understanding hearing impairment and the development of future clinical interventions. Here, we obtained neural responses to an extensive set of natural vowel-consonant-vowel phoneme sequences, each produced by multiple talkers, in three stages of the auditory processing pathway. Auditory nerve (AN) representations were simulated using a model of the peripheral auditory system and extracellular neuronal activity was recorded in the inferior colliculus (IC) and primary auditory cortex (AI) of anaesthetized Guinea pigs. A classifier was developed to examine the efficacy of these representations for recognizing the speech sounds. Individual neurons convey progressively less information from AN to AI. Nonetheless, at the population level, representations are sufficiently rich to facilitate recognition of consonants with a high degree of accuracy at all stages indicating a progression from a dense, redundant representation to a sparse, distributed one. We examined the timescale of the neural code for consonant recognition and found that optimal timescales increase throughout the ascending auditory system from a few milliseconds in the periphery to several tens of milliseconds in the cortex. Despite these longer timescales, we found little evidence to suggest that representations up to the level of AI become increasingly invariant to across-talker differences. Instead, our results support the idea that the role of the subcortical auditory system is one of dimensionality expansion, which could provide a basis for flexible classification of arbitrary speech sounds.", "AuthorKeywords": ["Auditory cortex", "Auditory nerve", "Inferior colliculus", "Neural coding", "Speech processing", "Spike timing"], "IndexKeywords": null, "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-85055184025", "SubjectAreas": [["Neuroscience (all)", "NEUR", "2800"]], "AuthorData": {"57202967291": {"Name": "Steadman M.", "AuthorID": "57202967291", "AffiliationID": "60015150", "AffiliationName": "Department of Bioengineering, Imperial College London"}, "16508104900": {"Name": "Sumner C.", "AuthorID": "16508104900", "AffiliationID": "60015138, 60012900", "AffiliationName": "MRC Institute of Hearing Research, School of Medicine, University of Nottingham"}}}