{"Title": "A Family of Droids-Android Malware Detection via Behavioral Modeling: Static vs Dynamic Analysis", "Year": 2018, "Source": "Annu. Conf. Priv., Secur. Trust, PST", "Volume": null, "Issue": null, "Art.No": null, "PageStart": null, "PageEnd": null, "CitedBy": 12, "DOI": "10.1109/PST.2018.8514191", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85061760302&origin=inward", "Abstract": "\u00a9 2018 IEEE.Following the increasing popularity of the mobile ecosystem, cybercriminals have increasingly targeted mobile ecosystems, designing and distributing malicious apps that steal information or cause harm to the device's owner. Aiming to counter them, detection techniques based on either static or dynamic analysis that model Android malware, have been proposed. While the pros and cons of these analysis techniques are known, they are usually compared in the context of their limitations e.g., static analysis is not able to capture runtime behaviors, full code coverage is usually not achieved during dynamic analysis, etc. Whereas, in this paper, we analyze the performance of static and dynamic analysis methods in the detection of Android malware and attempt to compare them in terms of their detection performance, using the same modeling approach.To this end, we build on MAMADROID, a state-of-the-art detection system that relies on static analysis to create a behavioral model from the sequences of abstracted API calls. Then, aiming to apply the same technique in a dynamic analysis setting, we modify CHIMP, a platform recently proposed to crowdsource human inputs for app testing, in order to extract API calls' sequences from the traces produced while executing the app on a CHIMP virtual device. We call this system AUNTIEDROID and instantiate it by using both automated (Monkey) and usergenerated inputs. We find that combining both static and dynamic analysis yields the best performance, with F-measure reaching 0.92. We also show that static analysis is at least as effective as dynamic analysis, depending on how apps are stimulated during execution, and investigate the reasons for inconsistent misclassifications across methods.", "AuthorKeywords": null, "IndexKeywords": ["Analysis techniques", "Behavioral model", "Detection performance", "Detection system", "Misclassifications", "Runtime behaviors", "State of the art", "Static and dynamic analysis"], "DocumentType": "Conference Proceeding", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-85061760302", "SubjectAreas": [["Computer Networks and Communications", "COMP", "1705"], ["Information Systems and Management", "DECI", "1802"], ["Safety, Risk, Reliability and Quality", "ENGI", "2213"]], "AuthorData": {"57188644050": {"Name": "Onwuzurike L.", "AuthorID": "57188644050", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "57189589235": {"Name": "Mariconti E.", "AuthorID": "57189589235", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "36810977700": {"Name": "Stringhini G.", "AuthorID": "36810977700", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "17433897300": {"Name": "De Cristofaro E.", "AuthorID": "17433897300", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "55584406900": {"Name": "Almeida M.", "AuthorID": "55584406900", "AffiliationID": "60007592", "AffiliationName": "Polytechnic University of Catalonia"}, "25630344400": {"Name": "Blackburn J.", "AuthorID": "25630344400", "AffiliationID": "60027086", "AffiliationName": "University of Alabama at Birmingham"}}}