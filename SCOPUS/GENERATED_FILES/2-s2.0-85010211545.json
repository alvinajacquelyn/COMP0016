{"Title": "Facial Affect 'In-the-Wild': A Survey and a New Database", "Year": 2016, "Source": "IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn. Workshops", "Volume": null, "Issue": null, "Art.No": null, "PageStart": 1487, "PageEnd": 1498, "CitedBy": 20, "DOI": "10.1109/CVPRW.2016.186", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85010211545&origin=inward", "Abstract": "\u00a9 2016 IEEE.Well-established databases and benchmarks have been developed in the past 20 years for automatic facial behaviour analysis. Nevertheless, for some important problems regarding analysis of facial behaviour, such as (a) estimation of affect in a continuous dimensional space (e.g., valence and arousal) in videos displaying spontaneous facial behaviour and (b) detection of the activated facial muscles (i.e., facial action unit detection), to the best of our knowledge, well-established in-the-wild databases and benchmarks do not exist. That is, the majority of the publicly available corpora for the above tasks contain samples that have been captured in controlled recording conditions and/or captured under a very specific milieu. Arguably, in order to make further progress in automatic understanding of facial behaviour, datasets that have been captured in in the-wild and in various milieus have to be developed. In this paper, we survey the progress that has been recently made on understanding facial behaviour in-the-wild, the datasets that have been developed so far and the methodologies that have been developed, paying particular attention to deep learning techniques for the task. Finally, we make a significant step further and propose a new comprehensive benchmark for training methodologies, as well as assessing the performance of facial affect/behaviour analysis/ understanding in-the-wild. To the best of our knowledge, this is the first time that such a benchmark for valence and arousal 'in-the-wild' is presented.", "AuthorKeywords": null, "IndexKeywords": ["Automatic understanding", "Deep learning", "Dimensional spaces", "Facial action", "Facial behaviours", "Facial muscles"], "DocumentType": "Conference Proceeding", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-85010211545", "SubjectAreas": [["Computer Vision and Pattern Recognition", "COMP", "1707"], ["Electrical and Electronic Engineering", "ENGI", "2208"]], "AuthorData": {"8883680000": {"Name": "Zafeiriou S.", "AuthorID": "8883680000", "AffiliationID": "60012954", "AffiliationName": "Center for Machine Vision and Signal Analysis, University of Oulu"}, "55908612300": {"Name": "Papaioannou A.", "AuthorID": "55908612300", "AffiliationID": "60015150", "AffiliationName": "Imperial College London"}, "16030934400": {"Name": "Kotsia I.", "AuthorID": "16030934400", "AffiliationID": "60158100", "AffiliationName": "International Hellenic University"}, "36622278100": {"Name": "Nicolaou M.", "AuthorID": "36622278100", "AffiliationID": "60010964", "AffiliationName": "Goldsmiths, University of London"}, "47661917700": {"Name": "Zhao G.", "AuthorID": "47661917700", "AffiliationID": "60012954", "AffiliationName": "Center for Machine Vision and Signal Analysis, University of Oulu"}}}