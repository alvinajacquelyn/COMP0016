{"Title": "Cohesion and joint speech: Right hemisphere contributions to synchronized vocal production", "Year": 2016, "Source": "J. Neurosci.", "Volume": "36", "Issue": 17, "Art.No": null, "PageStart": 4669, "PageEnd": 4680, "CitedBy": 11, "DOI": "10.1523/JNEUROSCI.4075-15.2016", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84966458795&origin=inward", "Abstract": "\u00a9 2016 the authors.Synchronized behavior (chanting, singing, praying, dancing) is found in all human cultures and is central to religious, military, and political activities, which require people to act collaboratively and cohesively; however, we know little about the neural underpinnings of many kinds of synchronous behavior (e.g., vocal behavior) or its role in establishing and maintaining group cohesion. In the present study, we measured neural activity using fMRI while participants spoke simultaneously with another person. We manipulated whether the couple spoke the same sentence (allowing synchrony) or different sentences (preventing synchrony), and also whether the voice the participant heard was \u201clive\u201d (allowing rich reciprocal interaction) or prerecorded (with no such mutual influence). Synchronous speech was associated with increased activity in posterior and anterior auditory fields. When, and only when, participants spoke with a partner who was both synchronous and \u201clive,\u201d we observed a lack of the suppression of auditory cortex, which is commonly seen as a neural correlate of speech production. Instead, auditory cortex responded as though it were processing another talker's speech. Our results suggest that detecting synchrony leads to a change in the perceptual consequences of one's own actions: they are processed as though they were other-, rather than self-produced. This may contribute to our understanding of synchronized behavior as a group-bonding tool.", "AuthorKeywords": ["Coordinated action", "FMRI", "Joint speech", "Right hemisphere", "Social cohesion", "Speech control"], "IndexKeywords": ["Acoustic Stimulation", "Adult", "Auditory Cortex", "Auditory Perception", "Brain", "Brain Mapping", "Female", "Humans", "Image Processing, Computer-Assisted", "Magnetic Resonance Imaging", "Male", "Social Perception", "Speech", "Speech Perception"], "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-84966458795", "SubjectAreas": [["Neuroscience (all)", "NEUR", "2800"]], "AuthorData": {"36608155300": {"Name": "Jasmin K.M.", "AuthorID": "36608155300", "AffiliationID": "60003158, 60006577", "AffiliationName": "Laboratory of Brain and Cognition, National Institute of Mental Health, National Institutes of Health"}, "8358934300": {"Name": "McGettigan C.", "AuthorID": "8358934300", "AffiliationID": "60020595", "AffiliationName": "Department of Psychology, University of London"}, "7401505962": {"Name": "Scott S.K.", "AuthorID": "7401505962", "AffiliationID": "60109771, 60022148", "AffiliationName": "Institute of Cognitive Neuroscience, University College London"}, "56366640300": {"Name": "Lavan N.", "AuthorID": "56366640300", "AffiliationID": "60020595", "AffiliationName": "Department of Psychology, University of London"}, "24729331900": {"Name": "Agnew Z.K.", "AuthorID": "24729331900", "AffiliationID": "60023691", "AffiliationName": "Department of Otolaryngology, University of California\u2013San Francisco"}, "6701691307": {"Name": "Josephs O.", "AuthorID": "6701691307", "AffiliationID": "60019953, 60022148", "AffiliationName": "Institute of Neurology, University College London"}, "6701674377": {"Name": "Cummins F.", "AuthorID": "6701674377", "AffiliationID": "60005141", "AffiliationName": "School of Computer Science, University College Dublin"}}}