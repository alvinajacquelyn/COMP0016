{"Title": "Nesterov's accelerated gradient and momentum as approximations to regularised update descent", "Year": 2017, "Source": "Proc Int Jt Conf Neural Networks", "Volume": "2017-May", "Issue": null, "Art.No": null, "PageStart": 1899, "PageEnd": 1903, "CitedBy": 33, "DOI": "10.1109/IJCNN.2017.7966082", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85031012662&origin=inward", "Abstract": "\u00a9 2017 IEEE.We present a unifying framework for adapting the update direction in gradient-based iterative optimization methods. As natural special cases we re-derive classical momentum and Nesterov's accelerated gradient method, lending a new intuitive interpretation to the latter algorithm. We show that a new algorithm, which we term Regularised Gradient Descent, can converge more quickly than either Nesterov's algorithm or the classical momentum algorithm.", "AuthorKeywords": null, "IndexKeywords": ["Gradient based", "Gradient descent", "Iterative Optimization"], "DocumentType": "Conference Proceeding", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-85031012662", "SubjectAreas": [["Software", "COMP", "1712"], ["Artificial Intelligence", "COMP", "1702"]], "AuthorData": {"57196024416": {"Name": "Botev A.", "AuthorID": "57196024416", "AffiliationID": "60022148, 60111768", "AffiliationName": "Department of Computer Science, University College London, Alan Turing Institute"}, "6603227381": {"Name": "Lever G.", "AuthorID": "6603227381", "AffiliationID": "60022148, 60111768", "AffiliationName": "Department of Computer Science, University College London, Alan Turing Institute"}, "24921457500": {"Name": "Barber D.", "AuthorID": "24921457500", "AffiliationID": "60022148, 60111768", "AffiliationName": "Department of Computer Science, University College London, Alan Turing Institute"}}}