{"Title": "Joint material and illumination estimation from photo sets in the wild", "Year": 2018, "Source": "Proc. - Int. Conf. 3D Vis., 3DV", "Volume": null, "Issue": null, "Art.No": null, "PageStart": 22, "PageEnd": 31, "CitedBy": 9, "DOI": "10.1109/3DV.2018.00014", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85056800903&origin=inward", "Abstract": "\u00a9 2018 IEEE.Faithful manipulation of shape, material, and illumination in 2D Internet images would greatly benefit from a reliable factorization of appearance into material (i.e. diffuse and specular) and illumination (i.e. environment maps). On the one hand, current methods that produce very high fidelity results, typically require controlled settings, expensive devices, or significant manual effort. To the other hand, methods that are automatic and work on 'in the wild' Internet images, often extract only low-frequency lighting or diffuse materials. In this work, we propose to make use of a set of photographs in order to jointly estimate the non-diffuse materials and sharp lighting in an uncontrolled setting. Our key observation is that seeing multiple instances of the same material under different illumination (i.e. environment), and different materials under the same illumination provide valuable constraints that can be exploited to yield a high-quality solution (i.e. specular materials and environment illumination) for all the observed materials and environments. Similar constraints also arise when observing multiple materials in a single environment, or a single material across multiple environments. Technically, we enable this by a novel scalable formulation using parametric mixture models that allows for simultaneous estimation of all materials and illumination directly from a set of (uncontrolled) Internet images. The core of this approach is an optimization procedure that uses two neural networks that are trained on synthetic images to predict good gradients in parametric space given observation of reflected light. We evaluate our method on a range of synthetic and real examples to generate high-quality estimates, qualitatively compare our results against state-of-the-art alternatives via a user study, and demonstrate photo-consistent image manipulation that is otherwise very challenging to achieve.", "AuthorKeywords": ["Differentiable rendering", "Illumination estimation", "Material estimation"], "IndexKeywords": ["Differentiable rendering", "Environment illumination", "High-quality solutions", "Illumination estimation", "Multiple materials", "Optimization procedures", "Parametric mixture model", "Simultaneous estimation"], "DocumentType": "Conference Proceeding", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-85056800903", "SubjectAreas": [["Artificial Intelligence", "COMP", "1702"], ["Computer Science Applications", "COMP", "1706"], ["Computer Vision and Pattern Recognition", "COMP", "1707"]], "AuthorData": {"57204705870": {"Name": "Wang T.", "AuthorID": "57204705870", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "24537726600": {"Name": "Ritschel T.", "AuthorID": "24537726600", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "7103161991": {"Name": "Mitra N.", "AuthorID": "7103161991", "AffiliationID": "60022148", "AffiliationName": "University College London"}}}