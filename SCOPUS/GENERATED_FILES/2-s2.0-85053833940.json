{"Title": "Can surgical simulation be used to train detection and classification of neural networks?", "Year": 2017, "Source": "Healthc. Technol. Lett.", "Volume": "4", "Issue": 5, "Art.No": null, "PageStart": 216, "PageEnd": 222, "CitedBy": 10, "DOI": "10.1049/htl.2017.0064", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85053833940&origin=inward", "Abstract": "Computer-assisted interventions (CAI) aim to increase the effectiveness, precision and repeatability of procedures to improve surgical outcomes. The presence and motion of surgical tools is a key information input for CAI surgical phase recognition algorithms. Vision-based tool detection and recognition approaches are an attractive solution and can be designed to take advantage of the powerful deep learning paradigm that is rapidly advancing image recognition and classification. The challenge for such algorithms is the availability and quality of labelled data used for training. In this Letter, surgical simulation is used to train tool detection and segmentation based on deep convolutional neural networks and generative adversarial networks. The authors experiment with two network architectures for image segmentation in tool classes commonly encountered during cataract surgery. A commercially-available simulator is used to create a simulated cataract dataset for training models prior to performing transfer learning on real surgical data. To the best of authors' knowledge, this is the first attempt to train deep learning models for surgical instrument detection on simulated data while demonstrating promising results to generalise on real data. Results indicate that simulated data does have some potential for training advanced classification methods for CAI systems.", "AuthorKeywords": null, "IndexKeywords": ["Adversarial networks", "Attractive solutions", "Cataract surgeries", "Classification methods", "Deep convolutional neural networks", "Recognition algorithm", "Surgical instrument", "Surgical simulation"], "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-85053833940", "SubjectAreas": [["Health Informatics", "MEDI", "2718"], ["Health Information Management", "HEAL", "3605"]], "AuthorData": {"57203973800": {"Name": "Zisimopoulos O.", "AuthorID": "57203973800", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "57203974017": {"Name": "Flouty E.", "AuthorID": "57203974017", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "57210437557": {"Name": "Stacey M.", "AuthorID": "57210437557", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "53984882900": {"Name": "Muscroft S.", "AuthorID": "53984882900", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "50661430400": {"Name": "Giataganas P.", "AuthorID": "50661430400", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "37031585700": {"Name": "Nehme J.", "AuthorID": "37031585700", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "23388532000": {"Name": "Chow A.", "AuthorID": "23388532000", "AffiliationID": "121384632", "AffiliationName": "Touch Surgery, Kinosis, Ltd"}, "57217833493": {"Name": "Stoyanov D.", "AuthorID": "57217833493", "AffiliationID": "60022148", "AffiliationName": "Computer Science, University College London"}}}