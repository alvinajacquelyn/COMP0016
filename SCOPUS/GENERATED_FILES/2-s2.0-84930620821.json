{"Title": "QuantiFly: Robust trainable software for automated Drosophila egg counting", "Year": 2015, "Source": "PLoS ONE", "Volume": "10", "Issue": 5, "Art.No": null, "PageStart": null, "PageEnd": null, "CitedBy": 14, "DOI": "10.1371/journal.pone.0127659", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84930620821&origin=inward", "Abstract": "\u00a9 2015 Waithe et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.We report the development and testing of software called QuantiFly: an automated tool to quantify Drosophila egg laying. Many laboratories count Drosophila eggs as a marker of fitness. The existing method requires laboratory researchers to count eggs manually while looking down a microscope. This technique is both time-consuming and tedious, especially when experiments require daily counts of hundreds of vials. The basis of the QuantiFly software is an algorithm which applies and improves upon an existing advanced pattern recognition and machine-learning routine. The accuracy of the baseline algorithm is additionally increased in this study through correction of bias observed in the algorithm output. The QuantiFly software, which includes the refined algorithm, has been designed to be immediately accessible to scientists through an intuitive and responsive user-friendly graphical interface. The software is also open-source, self-contained, has no dependencies and is easily installed (https://github.com/dwaithe/quantifly). Compared to manual egg counts made from digital images, QuantiFly achieved average accuracies of 94% and 85% for eggs laid on transparent (defined) and opaque (yeast-based) fly media. Thus, the software is capable of detecting experimental differences in most experimental situations. Significantly, the advanced feature recognition capabilities of the software proved to be robust to food surface artefacts like bubbles and crevices. The user experience involves image acquisition, algorithm training by labelling a subset of eggs in images of some of the vials, followed by a batch analysis mode in which new images are automatically assessed for egg numbers. Initial training typically requires approximately 10 minutes, while subsequent image evaluation by the software is performed in just a few seconds. Given the average time per vial for manual counting is approximately 40 seconds, our software introduces a timesaving advantage for experiments starting with as few as 20 vials. We also describe an optional acrylic box to be used as a digital camera mount and to provide controlled lighting during image acquisition which will guarantee the conditions used in this study.", "AuthorKeywords": null, "IndexKeywords": ["Algorithms", "Animals", "Drosophila", "Image Processing, Computer-Assisted", "Molecular Sequence Data", "Oviposition", "Software", "Time Factors"], "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-84930620821", "SubjectAreas": [["Biochemistry, Genetics and Molecular Biology (all)", "BIOC", "1300"], ["Agricultural and Biological Sciences (all)", "AGRI", "1100"], ["Multidisciplinary", "MULT", "1000"]], "AuthorData": {"25626924900": {"Name": "Waithe D.", "AuthorID": "25626924900", "AffiliationID": "60008323, 60001633", "AffiliationName": "Wolfson Imaging Centre, Weatherall Institute of Molecular Medicine, John Radcliffe Hospital"}, "56673880800": {"Name": "Rennert P.", "AuthorID": "56673880800", "AffiliationID": "60022148", "AffiliationName": "Department of Computer Science, University College London"}, "15135206900": {"Name": "Brostow G.", "AuthorID": "15135206900", "AffiliationID": "60022148", "AffiliationName": "Department of Computer Science, University College London"}, "7005784113": {"Name": "Piper M.D.W.", "AuthorID": "7005784113", "AffiliationID": "60022148", "AffiliationName": "Institute of Healthy Ageing, Department of Genetics, Evolution and Environment, University College London"}}}