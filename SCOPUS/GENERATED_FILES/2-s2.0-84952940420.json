{"Title": "Estimating state and parameters in state space models of spike trains", "Year": 2015, "Source": "Advanced State Space Methods for Neural and Clinical Data", "Volume": null, "Issue": null, "Art.No": null, "PageStart": 137, "PageEnd": 159, "CitedBy": 21, "DOI": "10.1017/CBO9781139941433.007", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84952940420&origin=inward", "Abstract": "\u00a9 Cambridge University Press 2015Introduction: State space models for neural population spike trains Neural computations at all scales of evolutionary and behavioural complexity are carried out by recurrently connected networks of neurons that communicate with each other, with neurons elsewhere in the brain, and with muscles through the firing of action potentials or \u201cspikes.\u201d To understand how nervous tissue computes, it is therefore necessary to understand how the spiking of neurons is shaped both by inputs to the network and by the recurrent action of existing network activity. Whereas most historical spike data were collected one neuron at a time, new techniques including silicon multielectrode array recording and scanning 2-photon, light-sheet or light-field fluorescence calcium imaging increasingly make it possible to record spikes from dozens, hundreds and potentially thousands of individual neurons simultaneously. These new data offer unprecedented empirical access to network computation, promising breakthroughs both in our understanding of neural coding and computation (Stevenson & Kording 2011), and our ability to build prosthetic neural interfaces (Santhanam et al. 2006). Fulfillment of this promise will require powerful methods for data modeling and analysis, able to capture the structure of statistical dependence of network activity across neurons and time. Probabilistic latent state space models (SSMs) are particularly well-suited to this task. Neural activity often appears stochastic, in that repeated trials under the same controlled experimental conditions can evoke quite different patterns of firing. Some part of this variation may reflect differences in the way the computation unfolds on each trial. Another part might reflect noisy creation and transmission of neural signals. Yet more may come from chaotic amplification of small perturbations. As computational signals are thought to be distributed across the population (in a \u201cpopulation code\u201d), variation in the computation may be distinguished by its common impact on different neurons and the systematic evolution of these common effects in time. An SSM is able to capture such structured variation through the evolution of its latent state trajectory. This latent state provides a summary description of all factors modulating neural activity that are not observed directly. These factors could include processes such as arousal, attention, cortical state (Harris & Thiele 2011) or behavioural states of the animal (Niell & Stryker 2010; Maimon 2011).", "AuthorKeywords": null, "IndexKeywords": ["Experimental conditions", "Multielectrode arrays", "Network activities", "Network computations", "Neural computations", "Small perturbations", "State - space models", "Statistical dependence"], "DocumentType": "Book", "PublicationStage": null, "OpenAccess": 2, "EID": "2-s2.0-84952940420", "SubjectAreas": [["Engineering (all)", "ENGI", "2200"]], "AuthorData": {"55881003300": {"Name": "Macke J.", "AuthorID": "55881003300", "AffiliationID": "60006089", "AffiliationName": "Max Planck Institute"}, "23003340800": {"Name": "Buesing L.", "AuthorID": "23003340800", "AffiliationID": "60022148", "AffiliationName": "University College London"}, "7005205264": {"Name": "Sahani M.", "AuthorID": "7005205264", "AffiliationID": "60022148", "AffiliationName": "University College London"}}}