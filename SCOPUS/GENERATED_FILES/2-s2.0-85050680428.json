{"Title": "Language experience influences audiovisual speech integration in unimodal and bimodal bilingual infants", "Year": 2019, "Source": "Dev. Sci.", "Volume": "22", "Issue": 1, "Art.No": null, "PageStart": null, "PageEnd": null, "CitedBy": 7, "DOI": "10.1111/desc.12701", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85050680428&origin=inward", "Abstract": "\u00a9 2018 The Authors. Developmental Science Published by John Wiley & Sons Ltd.Infants as young as 2 months can integrate audio and visual aspects of speech articulation. A shift of attention from the eyes towards the mouth of talking faces occurs around 6 months of age in monolingual infants. However, it is unknown whether this pattern of attention during audiovisual speech processing is influenced by speech and language experience in infancy. The present study investigated this question by analysing audiovisual speech processing in three groups of 4- to 8-month-old infants who differed in their language experience: monolinguals, unimodal bilinguals (infants exposed to two or more spoken languages) and bimodal bilinguals (hearing infants with Deaf mothers). Eye-tracking was used to study patterns of face scanning while infants were viewing faces articulating syllables with congruent, incongruent and silent auditory tracks. Monolinguals and unimodal bilinguals increased their attention to the mouth of talking faces between 4 and 8 months, while bimodal bilinguals did not show any age difference in their scanning patterns. Moreover, older (6.6 to 8 months), but not younger, monolinguals (4 to 6.5 months) showed increased visual attention to the mouth of faces articulating audiovisually incongruent rather than congruent faces, indicating surprise or novelty. In contrast, no audiovisual congruency effect was found in unimodal or bimodal bilinguals. Results suggest that speech and language experience influences audiovisual integration in infancy. Specifically, reduced or more variable experience of audiovisual speech from the primary caregiver may lead to less sensitivity to the integration of audio and visual cues of speech articulation.", "AuthorKeywords": null, "IndexKeywords": ["Adult", "Attention", "Cues", "Eye Movements", "Face", "Female", "Humans", "Infant", "Male", "Mouth", "Multilingualism", "Speech Perception", "Visual Perception"], "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-85050680428", "SubjectAreas": [["Developmental and Educational Psychology", "PSYC", "3204"], ["Cognitive Neuroscience", "NEUR", "2805"]], "AuthorData": {"14827234800": {"Name": "Mercure E.", "AuthorID": "14827234800", "AffiliationID": "60109771", "AffiliationName": "UCL Institute of Cognitive Neuroscience"}, "57203148460": {"Name": "Goldberg L.", "AuthorID": "57203148460", "AffiliationID": "60109771", "AffiliationName": "UCL Institute of Cognitive Neuroscience"}, "57203141368": {"Name": "Bowden-Howl H.", "AuthorID": "57203141368", "AffiliationID": "60024779", "AffiliationName": "School of Psychology, University of Plymouth"}, "57203149809": {"Name": "Coulson K.", "AuthorID": "57203149809", "AffiliationID": "60032760", "AffiliationName": "Department of Psychology and Sports Sciences, University of Hertfordshire"}, "6602263850": {"Name": "MacSweeney M.", "AuthorID": "6602263850", "AffiliationID": "60109771", "AffiliationName": "UCL Institute of Cognitive Neuroscience"}, "6602726543": {"Name": "Kushnerenko E.", "AuthorID": "6602726543", "AffiliationID": "60026116", "AffiliationName": "School of Psychology, University of East London"}, "36072829800": {"Name": "Johnson M.H.", "AuthorID": "36072829800", "AffiliationID": "60031101", "AffiliationName": "Department of Psychology, University of Cambridge"}}}