{"Title": "What is the value of embedding artificial emotional prosody in human-computer interactions? Implications for theory and design in psychological science", "Year": 2015, "Source": "Front. Psychol.", "Volume": "6", "Issue": null, "Art.No": null, "PageStart": null, "PageEnd": null, "CitedBy": 3, "DOI": "10.3389/fpsyg.2015.01750", "Link": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84949667804&origin=inward", "Abstract": "\u00a9 2015 Mitchell and Xu.In computerized technology, artificial speech is becoming increasingly important, and is already used in ATMs, online gaming and healthcare contexts. However, today's artificial speech typically sounds monotonous, a main reason for this being the lack of meaningful prosody. One particularly important function of prosody is to convey different emotions. This is because successful encoding and decoding of emotions is vital for effective social cognition, which is increasingly recognized in human-computer interaction contexts. Current attempts to artificially synthesize emotional prosody are much improved relative to early attempts, but there remains much work to be done due to methodological problems, lack of agreed acoustic correlates, and lack of theoretical grounding. If the addition of synthetic emotional prosody is not of sufficient quality, it may risk alienating users instead of enhancing their experience. So the value of embedding emotion cues in artificial speech may ultimately depend on the quality of the synthetic emotional prosody. However, early evidence on reactions to synthesized non-verbal cues in the facial modality bodes well. Attempts to implement the recognition of emotional prosody into artificial applications and interfaces have perhaps been met with greater success, but the ultimate test of synthetic emotional prosody will be to critically compare how people react to synthetic emotional prosody vs. natural emotional prosody, at the behavioral, socio-cognitive and neural levels.", "AuthorKeywords": ["Artificial speech", "Emotion", "Human-computer interaction", "Prosody", "Social cognition"], "IndexKeywords": null, "DocumentType": "Journal", "PublicationStage": null, "OpenAccess": 1, "EID": "2-s2.0-84949667804", "SubjectAreas": [["Psychology (all)", "PSYC", "3200"]], "AuthorData": {"7403974582": {"Name": "Mitchell R.", "AuthorID": "7403974582", "AffiliationID": "60011520", "AffiliationName": "Centre for Affective Disorders, Institute of Psychiatry Psychology and Neuroscience, King's College London"}, "55695016900": {"Name": "Xu Y.", "AuthorID": "55695016900", "AffiliationID": "60022148", "AffiliationName": "Speech Hearing and Phonetic Sciences, Division of Psychology and Language Sciences, University College London"}}}